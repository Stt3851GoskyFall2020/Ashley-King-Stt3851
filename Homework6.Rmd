---
title: "Homework 6"
author: "Ashley King"
date: "4/25/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
library(ISLR)
library(pls)
grid=10^seq(10,-2,length=100)
```


## 9. In this exercise, we will predict the number of applications received using the other variables in the College data set.

### (a) Split the data set into a training set and a test set.

```{r}
set.seed(111)
total_length = nrow(College)
train = sample(total_length, total_length * 0.70)
training_data = College[train,]
test = College[-train,]
```


### (b) Fit a linear model using least squares on the training set, and report the test error obtained.

```{r}
college_model = lm(Apps ~ ., data = College, subset = train)
summary(college_model)
```

```{r}
linear_predicted = predict(college_model, test)
linear_error = mean((linear_predicted - test$Apps)^2)
linear_error
```

+ The test MSE for the linear model is `r linear_error`
+ This appears to be a high error, but we need to compare it to some other models to know.

### (c) Fit a ridge regression model on the training set, with λ chosen by cross-validation. Report the test error obtained.

```{r}
train_x = model.matrix(Apps~., training_data)
train_y = training_data$Apps
test_x = model.matrix(Apps~., test)
test_y = test$Apps

ridge.mod = glmnet(train_x, train_y, alpha = 0, lambda = grid, thresh = 1e-12)
```

```{r}
cv.out = cv.glmnet(train_x, train_y, alpha = 0)
bestlam = cv.out$lambda.min
bestlam
```

+ Our lambda value is `r bestlam`

```{r}
ridge.pred = predict(ridge.mod,s = bestlam,newx = test_x)

# Using the best-lambda chosen from cross validation we will calculate the test MSE for the test data.
ridge_error = mean((ridge.pred-test_y)^2)
ridge_error
```

+ The test MSE is `r ridge_error`
+ This model has a higher error than the linear model.

### (d) Fit a lasso model on the training set, with λ chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates.

```{r}
lasso.mod = glmnet(train_x, train_y, alpha = 1, lambda = grid, thresh = 1e-12)

cv.out = cv.glmnet(train_x, train_y, alpha = 1)

bestlam = cv.out$lambda.min
bestlam
```

```{r}
lasso.pred = predict(lasso.mod, s = bestlam, newx = test_x)
lasso_error = mean((lasso.pred-test_y)^2)
lasso_error
```

+ The test MSE is `r lasso_error`
+ Our lasso model performs worse than the linear model but better than the ridge model.

```{r}
x = model.matrix(Apps~.,College)[,-1]
y = College$Apps

out = glmnet(x, y, alpha = 1, lambda = bestlam)

lasso.coef = predict(out, type = "coefficients", s = bestlam)[1:18,]
lasso.coef[lasso.coef != 0]
```

+ These are the non-zero coefficient estimates, there are 10 total.

### (e) Fit a PCR model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of M selected by cross-validation.

```{r}
pcr_College = pcr(Apps ~ ., data = training_data,  scale = TRUE, validation = "CV")
summary(pcr_College)
```

```{r}
validationplot(pcr_College,val.type = "MSEP")
```

+ It appears that the best point is around 16-17, as those are the lowest points

```{r}
pcr.pred = predict(pcr_College, College[-train,], ncomp =  17)
pcr_error = mean((pcr.pred-test_y)^2) 
pcr_error
```

+ The PCR test MSE is `r pcr_error`
+ Although this outperforms the lasso and ridge model, it interestingly has the same performance as the linear model

### (f) Fit a PLS model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of M selected by cross-validation.

```{r}
pls.fit = plsr(Apps~., data = College, subset = train, scale = TRUE, validation = "CV")
summary(pls.fit)
```

```{r}
validationplot(pls.fit, val.type = "MSEP")
```

+ There is a drastic decrease at first, then levels off past 5
+ **The level of M I will use is 8**, since it levels off around then, and minimizes the number of components.

```{r}
pls.pred = predict(pls.fit, College[-train,], ncomp = 8)

pls_error = mean((pls.pred-test_y)^2)
pls_error
```

+ The PLS test MSE is `r pls_error`
+ The PLS model outperforms all other models.

### (g) Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?


```{r}
all_errors = cbind(linear_error, ridge_error, pls_error, lasso_error, pcr_error)
all_errors
```

+ Based on the results, we can conclude that the "ranking" of models are: 
    + 1. PLS: Lowest error rate of `r pls_error`
    + 2. Linear: Error rate of `r linear_error`
    + 3. PCR: Error rate of `r pcr_error`
    + 4. Lasso: Error rate of `r lasso_error`
    + 5. Ridge: Error rate of `r ridge_error`

There is not a large difference between lasso and ridge, but a large difference between the other models. The Linear and PCR model had the same error rate.

#### Calculating R-Squared values

+ Since I am more used to R-Squared values, I decided to manually calculate it


```{r}
rsq = function (predicted, actual) {
  return(1 - (sum((predicted - actual) ^ 2)/sum((actual - mean(actual)) ^ 2) ))
}

rsq(pls.pred, test_y)
rsq(linear_predicted, test_y)
rsq(pcr.pred, test_y)
rsq(lasso.pred, test_y)
rsq(ridge.pred, test_y)
```

+ All of our models perform well, with R-Squared accuracies if over 90%
+ Our models vary from 93% to 91.96% R-Squared accuracy